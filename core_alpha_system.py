"""
Core Alpha Factor System - NLP-based Quantitative Trading Signal Generation
Focuses on 5 core text factors: sentiment momentum, sentiment reversal, news anomaly, text momentum, sentiment divergence
"""

import pandas as pd
import numpy as np
import sqlite3
try:
    import yfinance as yf
except ImportError:  # pragma: no cover - fallback when yfinanceç¼ºå¤±
    yf = None
from datetime import datetime, timedelta
import requests
import time
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# NLP dependencies
try:
    import nltk
    from nltk.sentiment.vader import SentimentIntensityAnalyzer
except ImportError:  # pragma: no cover
    nltk = None
    SentimentIntensityAnalyzer = None
try:
    from transformers import BertTokenizer, BertForSequenceClassification
except ImportError:  # pragma: no cover
    BertTokenizer = None
    BertForSequenceClassification = None

try:
    import torch
except ImportError:  # pragma: no cover
    torch = None
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import logging


if nltk is not None and SentimentIntensityAnalyzer is not None:
    nltk.download('vader_lexicon', quiet=True)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseManager:
    """SQLiteæ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "financial_data.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        
        # ä»·æ ¼æ•°æ®è¡¨
        conn.execute("""
        CREATE TABLE IF NOT EXISTS stock_prices (
            date TEXT,
            ticker TEXT,
            open REAL,
            high REAL,
            low REAL,
            close REAL,
            volume INTEGER,
            adj_close REAL,
            PRIMARY KEY (date, ticker)
        )
        """)
        
        # æ–°é—»æ•°æ®è¡¨
        conn.execute("""
        CREATE TABLE IF NOT EXISTS news_data (
            id INTEGER PRIMARY KEY,
            date TEXT,
            ticker TEXT,
            title TEXT,
            summary TEXT,
            url TEXT,
            source TEXT,
            sentiment_score REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # Alphaå› å­è¡¨
        conn.execute("""
        CREATE TABLE IF NOT EXISTS alpha_factors (
            date TEXT,
            ticker TEXT,
            sentiment_momentum REAL,
            sentiment_reversal REAL,
            news_volume_anomaly REAL,
            text_momentum REAL,
            sentiment_divergence REAL,
            combined_alpha REAL,
            PRIMARY KEY (date, ticker)
        )
        """)
        
        conn.commit()
        conn.close()
        logger.info("Database initialized successfully")

class DataCollector:
    """æ•°æ®æ”¶é›†å™¨ - æ•´åˆå¤šæºæ•°æ®"""

    def __init__(self, db_manager: DatabaseManager, alpha_vantage_key: str = "CR9P7L6SGO9W1L8V"):
        self.db = db_manager
        self.alpha_vantage_key = alpha_vantage_key

    # ---- è¾…åŠ©å‡½æ•° -------------------------------------------------

    @staticmethod
    def _normalize_price_dates(df: pd.DataFrame) -> pd.DataFrame:
        """å°†æ—¥æœŸåˆ—ç»Ÿä¸€ä¸ºæ— æ—¶åŒºçš„YYYY-MM-DDæ–‡æœ¬"""
        if 'date' in df.columns:
            date_series = pd.to_datetime(df['date'], errors='coerce')
        elif 'Date' in df.columns:
            date_series = pd.to_datetime(df['Date'], errors='coerce')
        else:
            return df

        date_series = date_series.dt.tz_localize(None)
        df['date'] = date_series.dt.strftime('%Y-%m-%d')
        return df

    @staticmethod
    def _period_to_sessions(period: str) -> int:
        """å°†yfinanceå‘¨æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¤§è‡´çš„äº¤æ˜“æ—¥æ•°é‡"""
        mapping = {
            '1y': 252,
            '2y': 252 * 2,
            '3y': 252 * 3,
            '6mo': 126,
            '1mo': 21,
        }
        return mapping.get(period.lower(), 252)

    def _generate_synthetic_price_data(self, ticker: str, period: str = "1y") -> Optional[pd.DataFrame]:
        """åœ¨ç½‘ç»œä¸å¯ç”¨æ—¶ç”Ÿæˆç¦»çº¿æ¼”ç¤ºæ•°æ®"""
        try:
            sessions = self._period_to_sessions(period)
            end_date = datetime.utcnow().date()
            start_sessions = max(sessions, 60)
            # ä½¿ç”¨äº¤æ˜“æ—¥é¢‘ç‡
            date_index = pd.bdate_range(end=end_date, periods=start_sessions)

            rng = np.random.default_rng(abs(hash(ticker)) % (2 ** 32))
            # äº§ç”Ÿéšæœºå¯¹æ•°æ”¶ç›Šï¼Œæ§åˆ¶æ³¢åŠ¨åœ¨2%
            log_returns = rng.normal(loc=0.0003, scale=0.02, size=len(date_index))
            prices = 100 * np.exp(np.cumsum(log_returns))

            df = pd.DataFrame({
                'Date': date_index,
                'Open': prices * (1 + rng.normal(0, 0.002, len(prices))),
                'High': prices * (1 + rng.normal(0.003, 0.002, len(prices))).clip(min=0),
                'Low': prices * (1 + rng.normal(-0.003, 0.002, len(prices))).clip(min=0),
                'Close': prices,
                'Volume': rng.integers(low=1_000_000, high=10_000_000, size=len(prices))
            })

            df['High'] = df[['Open', 'Close', 'High']].max(axis=1)
            df['Low'] = df[['Open', 'Close', 'Low']].min(axis=1)

            df = self._normalize_price_dates(df)
            df['adj_close'] = df['Close']
            df['ticker'] = ticker

            df = df.rename(columns={
                'Open': 'open',
                'High': 'high',
                'Low': 'low',
                'Close': 'close',
                'Volume': 'volume'
            })

            logger.warning(f"âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿä»·æ ¼æ•°æ®ç”¨äº {ticker} ({period})")
            return df[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close']]
        except Exception as e:
            logger.error(f"æ— æ³•ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®: {e}")
            return None
    
    def get_sp500_tickers(self, limit: int = 30) -> List[str]:
        """è·å–S&P 500æˆåˆ†è‚¡ä»£ç ï¼ˆdemoç‰ˆæœ¬é™åˆ¶æ•°é‡ï¼‰"""
        # é¢„å®šä¹‰çš„å¤§ç›˜è‚¡åˆ—è¡¨ï¼ˆå¯æ‰©å±•ï¼‰
        major_stocks = [
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'BRK-B',
            'JNJ', 'V', 'PG', 'JPM', 'UNH', 'HD', 'MA', 'DIS', 'PYPL', 'BAC',
            'NFLX', 'ADBE', 'CRM', 'CMCSA', 'XOM', 'VZ', 'ABT', 'PFE', 'T',
            'WMT', 'CSCO', 'PEP'
        ]
        return major_stocks[:limit]
    
    def collect_stock_data(self, tickers: List[str], period: str = "2y") -> bool:
        """æ‰¹é‡æ”¶é›†è‚¡ç¥¨ä»·æ ¼æ•°æ® - å¢å¼ºç‰ˆå¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†"""
        logger.info(f"Collecting price data for {len(tickers)} stocks")
        
        # ä½¿ç”¨WALæ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½
        conn = sqlite3.connect(self.db.db_path, timeout=30.0)
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")
        collected_frames = []
        successful_tickers = []
        failed_tickers = []

        if yf is None:
            logger.warning("yfinance æœªå®‰è£…ï¼Œæ‰€æœ‰ä»·æ ¼æ•°æ®å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

        for ticker in tickers:
            retries = 3 if yf is not None else 0
            success = False

            if yf is not None:
                for attempt in range(retries):
                    try:
                        logger.info(f"Fetching {ticker} (attempt {attempt + 1}/{retries})")
                        stock = yf.Ticker(ticker)
                        hist = stock.history(period=period, timeout=30)

                        if hist.empty or len(hist) < 50:
                            logger.warning(f"No sufficient data returned for {ticker} (records={len(hist)})")
                            break

                        data = hist.reset_index()
                        data['ticker'] = ticker
                        data = data.rename(columns={
                            'Date': 'date',
                            'Open': 'open',
                            'High': 'high',
                            'Low': 'low',
                            'Close': 'close',
                            'Volume': 'volume'
                        })
                        data = self._normalize_price_dates(data)
                        data['adj_close'] = data['close']
                        data = data.dropna()

                        if data.empty:
                            logger.warning(f"No valid data after cleaning for {ticker}")
                            break

                        collected_frames.append(data[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close']])
                        successful_tickers.append(ticker)
                        success = True
                        break

                    except Exception as e:
                        logger.error(f"âŒ Attempt {attempt + 1} failed for {ticker}: {e}")
                        if attempt < retries - 1:
                            wait_time = (attempt + 1) * 2
                            logger.info(f"Waiting {wait_time}s before retry...")
                            time.sleep(wait_time)

            if not success:
                synthetic = self._generate_synthetic_price_data(ticker, period)
                if synthetic is not None and not synthetic.empty:
                    collected_frames.append(synthetic)
                    successful_tickers.append(ticker)
                else:
                    failed_tickers.append(ticker)

            time.sleep(0.5)

        try:
            if collected_frames:
                combined_df = pd.concat(collected_frames, ignore_index=True)
                combined_df = combined_df.drop_duplicates(subset=['date', 'ticker'], keep='last')
                combined_df.to_sql('stock_prices', conn, if_exists='replace', index=False)
            conn.commit()
            logger.info(f"âœ… Data collection completed: {len(successful_tickers)} successful, {len(failed_tickers)} failed")
        except Exception as e:
            logger.error(f"âŒ Failed to commit data: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()

        if successful_tickers:
            logger.info(f"âœ… Successfully collected: {', '.join(successful_tickers)}")
        if failed_tickers:
            logger.warning(f"âš ï¸ Failed to collect: {', '.join(failed_tickers)}")

        return len(successful_tickers) > 0
    
    def check_news_cache(self, ticker: str, days: int = 7) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰æœ€è¿‘çš„æ–°é—»ç¼“å­˜"""
        conn = sqlite3.connect(self.db.db_path)
        try:
            cutoff_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            query = """
            SELECT COUNT(*) as count FROM news_data
            WHERE ticker = ? AND date >= ?
            """
            result = pd.read_sql(query, conn, params=[ticker, cutoff_date])
            return result['count'].iloc[0] > 0
        finally:
            conn.close()

    def collect_news_data(self, ticker: str, days: int = 30, force_refresh: bool = False) -> List[Dict]:
        """æ”¶é›†å•ä¸ªè‚¡ç¥¨çš„æ–°é—»æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and self.check_news_cache(ticker, days=7):
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ–°é—»æ•°æ®: {ticker}")
            return []

        try:
            logger.info(f"ğŸ“° æ­£åœ¨ä» Alpha Vantage è·å– {ticker} çš„æ–°é—»...")
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': ticker,
                'apikey': self.alpha_vantage_key,
                'limit': 200
            }

            response = requests.get(url, params=params, timeout=15)

            if response.status_code != 200:
                logger.error(f"API è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []

            data = response.json()

            # æ£€æŸ¥ API é™åˆ¶
            if 'Note' in data:
                logger.warning(f"âš ï¸ API é™åˆ¶: {data['Note']}")
                return []

            if 'Information' in data:
                logger.warning(f"âš ï¸ API ä¿¡æ¯: {data['Information']}")
                return []

            if 'feed' not in data:
                logger.error(f"æœªæ‰¾åˆ°æ–°é—»æ•°æ®: {data}")
                return []

            news_items = []
            for item in data['feed'][:100]:  # è·å–æ›´å¤šæ–°é—»
                # è§£ææ—¶é—´æˆ³: 20231215T153000 -> 2023-12-15
                time_published = item.get('time_published', '')
                if len(time_published) >= 8:
                    date_str = f"{time_published[:4]}-{time_published[4:6]}-{time_published[6:8]}"
                else:
                    date_str = datetime.now().strftime('%Y-%m-%d')

                # è·å–æƒ…æ„Ÿåˆ†æ•°
                ticker_sentiments = item.get('ticker_sentiment', [])
                sentiment_score = 0.0
                for ts in ticker_sentiments:
                    if ts.get('ticker') == ticker:
                        try:
                            sentiment_score = float(ts.get('ticker_sentiment_score', 0.0))
                        except (ValueError, TypeError):
                            sentiment_score = 0.0
                        break

                news_items.append({
                    'date': date_str,
                    'ticker': ticker,
                    'title': item.get('title', '')[:500],  # é™åˆ¶é•¿åº¦
                    'summary': item.get('summary', '')[:1000],
                    'url': item.get('url', ''),
                    'source': item.get('source', 'Unknown'),
                    'sentiment_score': sentiment_score
                })

            logger.info(f"âœ… æˆåŠŸè·å– {len(news_items)} æ¡æ–°é—»: {ticker}")
            return news_items

        except requests.exceptions.Timeout:
            logger.error(f"â° API è¯·æ±‚è¶…æ—¶: {ticker}")
            return []
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯ {ticker}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯ {ticker}: {e}")
            return []

    def batch_collect_news(self, tickers: List[str], force_refresh: bool = False) -> bool:
        """æ‰¹é‡æ”¶é›†å¤šä¸ªè‚¡ç¥¨çš„æ–°é—»æ•°æ®"""
        logger.info(f"ğŸ“° å¼€å§‹æ”¶é›† {len(tickers)} åªè‚¡ç¥¨çš„æ–°é—»æ•°æ®...")

        conn = sqlite3.connect(self.db.db_path, timeout=30.0)
        all_news = []
        successful_count = 0

        for i, ticker in enumerate(tickers):
            logger.info(f"[{i+1}/{len(tickers)}] æ­£åœ¨å¤„ç†: {ticker}")

            news_items = self.collect_news_data(ticker, force_refresh=force_refresh)

            if news_items:
                all_news.extend(news_items)
                successful_count += 1

            # API é™åˆ¶ï¼šæ¯åˆ†é’Ÿæœ€å¤š 5 æ¬¡è¯·æ±‚ï¼ˆå…è´¹ç‰ˆï¼‰
            if i < len(tickers) - 1:
                time.sleep(12)  # ç­‰å¾… 12 ç§’ï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶

        # ä¿å­˜åˆ°æ•°æ®åº“
        if all_news:
            try:
                news_df = pd.DataFrame(all_news)

                # å»é‡ï¼šåŸºäº ticker + title
                news_df = news_df.drop_duplicates(subset=['ticker', 'title'], keep='first')

                # æ£€æŸ¥å·²å­˜åœ¨çš„æ–°é—»
                existing_query = """
                SELECT ticker, title FROM news_data
                """
                existing_df = pd.read_sql(existing_query, conn)

                if not existing_df.empty:
                    # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ–°é—»
                    merged = news_df.merge(
                        existing_df,
                        on=['ticker', 'title'],
                        how='left',
                        indicator=True
                    )
                    news_df = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])

                if not news_df.empty:
                    news_df.to_sql('news_data', conn, if_exists='append', index=False)
                    logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(news_df)} æ¡æ–°é—»åˆ°æ•°æ®åº“")
                else:
                    logger.info("â„¹ï¸ æ²¡æœ‰æ–°çš„æ–°é—»éœ€è¦ä¿å­˜ï¼ˆå…¨éƒ¨å·²å­˜åœ¨ï¼‰")

                conn.commit()
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜æ–°é—»æ•°æ®å¤±è´¥: {e}")
                conn.rollback()
                return False
            finally:
                conn.close()

        logger.info(f"âœ… æ–°é—»æ”¶é›†å®Œæˆ: {successful_count}/{len(tickers)} æˆåŠŸ")
        return successful_count > 0

class NLPProcessor:
    """NLPå¤„ç†å™¨ - æ–‡æœ¬åˆ†æå’Œæƒ…æ„Ÿè®¡ç®—"""
    
    def __init__(self):
        # åˆå§‹åŒ–VADER
        if SentimentIntensityAnalyzer is not None:
            self.vader = SentimentIntensityAnalyzer()
        else:
            self.vader = None
            logger.warning("NLTK VADER æœªå®‰è£…ï¼Œæƒ…æ„Ÿåˆ†æå°†é€€åŒ–ä¸º0")
        
        # åˆå§‹åŒ–FinBERT
        if torch is None or BertTokenizer is None or BertForSequenceClassification is None:
            self.finbert_available = False
            self.finbert_tokenizer = None
            self.finbert_model = None
            logger.warning("FinBERTä¾èµ–æœªæ»¡è¶³ï¼ˆéœ€è¦torchå’Œtransformersï¼‰ï¼Œå°†ä½¿ç”¨ç®€åŒ–æƒ…æ„Ÿåˆ†æ")
        else:
            try:
                self.finbert_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')
                self.finbert_model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')
                self.finbert_available = True
                logger.info("FinBERT model loaded successfully")
            except Exception as exc:
                self.finbert_available = False
                self.finbert_tokenizer = None
                self.finbert_model = None
                logger.warning(f"FinBERT model not available ({exc}); using fallback sentiment")
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """ç»¼åˆæƒ…æ„Ÿåˆ†æ"""
        if not text:
            return {'compound': 0.0, 'confidence': 0.0}
        
        # VADERåˆ†æ
        if self.vader is not None:
            vader_scores = self.vader.polarity_scores(text)
        else:
            vader_scores = {'compound': 0.0}
        
        # FinBERTåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.finbert_available and self.finbert_model is not None and torch is not None:
            try:
                inputs = self.finbert_tokenizer(text, return_tensors='pt', 
                                              truncation=True, max_length=512)
                
                with torch.no_grad():
                    outputs = self.finbert_model(**inputs)
                    probabilities = torch.softmax(outputs.logits, dim=1)
                    
                # FinBERTè¾“å‡ºï¼š[negative, neutral, positive]
                finbert_score = probabilities[0][2].item() - probabilities[0][0].item()
                
                # ç»“åˆä¸¤ä¸ªåˆ†æ•°
                combined_score = (vader_scores['compound'] + finbert_score) / 2
                confidence = max(abs(vader_scores['compound']), abs(finbert_score))
                
            except:
                combined_score = vader_scores['compound']
                confidence = abs(vader_scores['compound'])
        else:
            combined_score = vader_scores['compound']
            confidence = abs(vader_scores['compound'])
        
        return {
            'compound': combined_score,
            'confidence': confidence
        }
    
    def extract_text_features(self, texts: List[str]) -> Dict[str, List[float]]:
        """æå–æ–‡æœ¬ç‰¹å¾ç”¨äºå› å­è®¡ç®—"""
        features = {
            'sentiment_scores': [],
            'confidence_scores': [],
            'text_lengths': [],
            'keyword_counts': [],
            'urgency_scores': []
        }
        
        # é‡‘èå…³é”®è¯
        financial_keywords = [
            'earnings', 'revenue', 'profit', 'loss', 'growth', 'decline',
            'bullish', 'bearish', 'buy', 'sell', 'upgrade', 'downgrade',
            'merger', 'acquisition', 'dividend', 'split', 'breakthrough'
        ]
        
        urgency_words = [
            'breaking', 'urgent', 'alert', 'emergency', 'crisis', 'surge',
            'plunge', 'spike', 'crash', 'soar', 'tumble'
        ]
        
        for text in texts:
            if not text:
                # å¡«å……é»˜è®¤å€¼
                features['sentiment_scores'].append(0.0)
                features['confidence_scores'].append(0.0)
                features['text_lengths'].append(0)
                features['keyword_counts'].append(0)
                features['urgency_scores'].append(0.0)
                continue
            
            # æƒ…æ„Ÿåˆ†æ
            sentiment = self.analyze_sentiment(text)
            features['sentiment_scores'].append(sentiment['compound'])
            features['confidence_scores'].append(sentiment['confidence'])
            
            # æ–‡æœ¬é•¿åº¦
            features['text_lengths'].append(len(text))
            
            # å…³é”®è¯è®¡æ•°
            text_lower = text.lower()
            keyword_count = sum(1 for keyword in financial_keywords if keyword in text_lower)
            features['keyword_counts'].append(keyword_count)
            
            # ç´§æ€¥æ€§è¯„åˆ†
            urgency_count = sum(1 for word in urgency_words if word in text_lower)
            urgency_score = min(urgency_count / 5.0, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1
            features['urgency_scores'].append(urgency_score)
        
        return features

class AlphaFactorEngine:
    """Alphaå› å­å¼•æ“ - ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
    
    def __init__(self, db_manager: DatabaseManager, nlp_processor: NLPProcessor):
        self.db = db_manager
        self.nlp = nlp_processor
        self.scaler = StandardScaler()
    
    def calculate_sentiment_momentum(self, ticker: str, lookback: int = 20) -> float:
        """æƒ…æ„ŸåŠ¨é‡å› å­ - æƒ…æ„Ÿå˜åŒ–ç‡"""
        conn = sqlite3.connect(self.db.db_path)
        
        query = """
        SELECT date, sentiment_score 
        FROM news_data 
        WHERE ticker = ? AND sentiment_score IS NOT NULL
        ORDER BY date DESC 
        LIMIT ?
        """
        
        df = pd.read_sql(query, conn, params=[ticker, lookback])
        conn.close()
        
        if len(df) < 5:
            return 0.0
        
        # è®¡ç®—æƒ…æ„ŸåŠ¨é‡ï¼ˆè¿‘æœŸvså†å²å¹³å‡ï¼‰
        recent_sentiment = df['sentiment_score'].head(5).mean()
        historical_sentiment = df['sentiment_score'].tail(15).mean()
        
        momentum = (recent_sentiment - historical_sentiment) / (abs(historical_sentiment) + 0.01)
        
        return np.clip(momentum, -3, 3)  # é™åˆ¶æç«¯å€¼
    
    def calculate_sentiment_reversal(self, ticker: str, threshold: float = 2.0) -> float:
        """æƒ…æ„Ÿåè½¬å› å­ - æç«¯æƒ…æ„Ÿåçš„åè½¬ä¿¡å·"""
        conn = sqlite3.connect(self.db.db_path)
        
        query = """
        SELECT sentiment_score 
        FROM news_data 
        WHERE ticker = ? AND sentiment_score IS NOT NULL
        ORDER BY date DESC 
        LIMIT 10
        """
        
        df = pd.read_sql(query, conn, params=[ticker])
        conn.close()
        
        if len(df) < 3:
            return 0.0
        
        recent_sentiment = df['sentiment_score'].iloc[0]
        avg_sentiment = df['sentiment_score'].mean()
        std_sentiment = df['sentiment_score'].std() + 0.01
        
        # Z-scoreæ ‡å‡†åŒ–
        z_score = (recent_sentiment - avg_sentiment) / std_sentiment
        
        # åè½¬ä¿¡å·ï¼šæç«¯æ­£é¢->è´Ÿä¿¡å·ï¼Œæç«¯è´Ÿé¢->æ­£ä¿¡å·
        if abs(z_score) > threshold:
            reversal_signal = -np.sign(z_score) * (abs(z_score) - threshold)
            return np.clip(reversal_signal, -2, 2)
        
        return 0.0
    
    def calculate_news_volume_anomaly(self, ticker: str, lookback: int = 30) -> float:
        """æ–°é—»é‡å¼‚å¸¸å› å­ - æ–°é—»æµé‡æ¿€å¢æ£€æµ‹"""
        conn = sqlite3.connect(self.db.db_path)
        
        # è·å–æ¯æ—¥æ–°é—»æ•°é‡
        query = """
        SELECT DATE(date) as day, COUNT(*) as news_count
        FROM news_data 
        WHERE ticker = ?
        GROUP BY DATE(date)
        ORDER BY day DESC 
        LIMIT ?
        """
        
        df = pd.read_sql(query, conn, params=[ticker, lookback])
        conn.close()
        
        if len(df) < 7:
            return 0.0
        
        recent_volume = df['news_count'].head(3).mean()
        historical_volume = df['news_count'].tail(20).mean()
        
        if historical_volume == 0:
            return 0.0
        
        volume_ratio = recent_volume / historical_volume
        
        # å¼‚å¸¸æ£€æµ‹ï¼šè¶…è¿‡å†å²å¹³å‡2å€è®¤ä¸ºæ˜¯å¼‚å¸¸
        if volume_ratio > 2.0:
            anomaly_score = min((volume_ratio - 2.0) / 2.0, 1.0)
            return anomaly_score
        
        return 0.0
    
    def calculate_text_momentum(self, ticker: str, lookback: int = 15) -> float:
        """æ–‡æœ¬åŠ¨é‡å› å­ - å…³é”®è¯é¢‘ç‡å˜åŒ–"""
        conn = sqlite3.connect(self.db.db_path)
        
        query = """
        SELECT date, title, summary 
        FROM news_data 
        WHERE ticker = ?
        ORDER BY date DESC 
        LIMIT ?
        """
        
        df = pd.read_sql(query, conn, params=[ticker, lookback * 2])
        conn.close()
        
        if len(df) < 10:
            return 0.0
        
        # åˆ†å‰²ä¸ºè¿‘æœŸå’Œå†å²
        recent_texts = df.head(lookback)['summary'].fillna('').tolist()
        historical_texts = df.tail(lookback)['summary'].fillna('').tolist()
        
        # æå–ç‰¹å¾
        recent_features = self.nlp.extract_text_features(recent_texts)
        historical_features = self.nlp.extract_text_features(historical_texts)
        
        # è®¡ç®—å…³é”®è¯é¢‘ç‡å˜åŒ–
        recent_keyword_freq = np.mean(recent_features['keyword_counts'])
        historical_keyword_freq = np.mean(historical_features['keyword_counts']) + 0.01
        
        momentum = (recent_keyword_freq - historical_keyword_freq) / historical_keyword_freq
        
        return np.clip(momentum, -2, 2)
    
    def calculate_sentiment_divergence(self, ticker: str, lookback: int = 15) -> float:
        """æƒ…æ„Ÿåˆ†æ­§åº¦å› å­ - å¸‚åœºæ„è§åˆ†æ­§ç¨‹åº¦"""
        conn = sqlite3.connect(self.db.db_path)
        
        query = """
        SELECT sentiment_score 
        FROM news_data 
        WHERE ticker = ? AND sentiment_score IS NOT NULL
        ORDER BY date DESC 
        LIMIT ?
        """
        
        df = pd.read_sql(query, conn, params=[ticker, lookback])
        conn.close()
        
        if len(df) < 5:
            return 0.0
        
        sentiments = df['sentiment_score'].values
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•£åº¦
        sentiment_std = np.std(sentiments)
        sentiment_range = np.max(sentiments) - np.min(sentiments)
        
        # åˆ†æ­§åº¦ï¼šæ ‡å‡†å·®å’ŒèŒƒå›´çš„ç»¼åˆ
        divergence = (sentiment_std + sentiment_range / 4.0) / 2.0
        
        return min(divergence, 2.0)  # é™åˆ¶æœ€å¤§å€¼
    
    def generate_combined_alpha(self, ticker: str) -> Dict[str, float]:
        """ç”Ÿæˆç»¼åˆAlphaä¿¡å·"""
        factors = {
            'sentiment_momentum': self.calculate_sentiment_momentum(ticker),
            'sentiment_reversal': self.calculate_sentiment_reversal(ticker),
            'news_volume_anomaly': self.calculate_news_volume_anomaly(ticker),
            'text_momentum': self.calculate_text_momentum(ticker),
            'sentiment_divergence': self.calculate_sentiment_divergence(ticker)
        }
        
        # æƒé‡é…ç½®ï¼ˆå¯è°ƒä¼˜ï¼‰
        weights = {
            'sentiment_momentum': 0.3,
            'sentiment_reversal': 0.25,
            'news_volume_anomaly': 0.2,
            'text_momentum': 0.15,
            'sentiment_divergence': 0.1
        }
        
        # è®¡ç®—åŠ æƒç»„åˆ
        combined_alpha = sum(factors[k] * weights[k] for k in factors.keys())
        factors['combined_alpha'] = combined_alpha
        
        return factors

class BacktestEngine:
    """ç®€åŒ–å›æµ‹å¼•æ“"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
    
    def calculate_factor_ic(self, lookback_days: int = 250) -> pd.DataFrame:
        """è®¡ç®—å› å­ä¿¡æ¯ç³»æ•°ï¼ˆICï¼‰"""
        conn = sqlite3.connect(self.db.db_path)
        
        # è·å–å› å­å’Œæ”¶ç›Šæ•°æ®
        query = """
        SELECT a.date, a.ticker, a.combined_alpha,
               p1.close as current_price,
               p2.close as next_price
        FROM alpha_factors a
        JOIN stock_prices p1 ON a.date = p1.date AND a.ticker = p1.ticker
        JOIN stock_prices p2 ON DATE(a.date, '+1 day') = p2.date AND a.ticker = p2.ticker
        ORDER BY a.date DESC
        LIMIT ?
        """
        
        df = pd.read_sql(query, conn, params=[lookback_days * 50])
        conn.close()
        
        if df.empty:
            return pd.DataFrame()
        
        # è®¡ç®—æœªæ¥æ”¶ç›Š
        df['future_return'] = (df['next_price'] - df['current_price']) / df['current_price']
        
        # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—IC
        ic_results = []
        for date, group in df.groupby('date'):
            if len(group) > 10:  # è‡³å°‘éœ€è¦10åªè‚¡ç¥¨
                ic = np.corrcoef(group['combined_alpha'], group['future_return'])[0, 1]
                if not np.isnan(ic):
                    ic_results.append({'date': date, 'ic': ic})
        
        ic_df = pd.DataFrame(ic_results)
        if not ic_df.empty:
            ic_df['date'] = pd.to_datetime(ic_df['date'])
        
        return ic_df
    
    def get_performance_summary(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        ic_df = self.calculate_factor_ic()
        
        if ic_df.empty:
            return {'ic_mean': 0, 'ic_std': 0, 'ir': 0, 'hit_rate': 0}
        
        ic_mean = ic_df['ic'].mean()
        ic_std = ic_df['ic'].std()
        ir = ic_mean / (ic_std + 0.001)  # Information Ratio
        hit_rate = (ic_df['ic'] > 0).mean()
        
        return {
            'ic_mean': ic_mean,
            'ic_std': ic_std,
            'ir': ir,
            'hit_rate': hit_rate
        }

def main_demo():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    logger.info("=== Alphaå› å­ç³»ç»ŸDemoå¯åŠ¨ ===")
    
    # åˆå§‹åŒ–ç»„ä»¶
    db = DatabaseManager()
    collector = DataCollector(db)
    nlp = NLPProcessor()
    alpha_engine = AlphaFactorEngine(db, nlp)
    backtest = BacktestEngine(db)
    
    # 1. æ•°æ®æ”¶é›†
    logger.info("Step 1: æ”¶é›†æ•°æ®")
    tickers = collector.get_sp500_tickers(limit=10)  # Demoç”¨10åªè‚¡ç¥¨
    logger.info(f"ç›®æ ‡è‚¡ç¥¨: {tickers}")
    
    # æ”¶é›†ä»·æ ¼æ•°æ®
    collector.collect_stock_data(tickers)

    # æ”¶é›†çœŸå®æ–°é—»æ•°æ®
    logger.info("æ”¶é›†çœŸå®æ–°é—»æ•°æ®...")
    collector.batch_collect_news(tickers[:5], force_refresh=False)  # å‰5åªè‚¡ç¥¨
    
    # 2. ç”ŸæˆAlphaå› å­
    logger.info("Step 2: ç”ŸæˆAlphaå› å­")
    conn = sqlite3.connect(db.db_path)
    
    alpha_results = []
    for ticker in tickers[:3]:  # Demoåªè®¡ç®—å‰3åª
        logger.info(f"è®¡ç®— {ticker} çš„Alphaå› å­")
        factors = alpha_engine.generate_combined_alpha(ticker)
        
        result = {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'ticker': ticker,
            **factors
        }
        alpha_results.append(result)
        
        logger.info(f"{ticker} Alphaä¿¡å·: {factors['combined_alpha']:.4f}")
    
    # ä¿å­˜Alphaå› å­
    pd.DataFrame(alpha_results).to_sql('alpha_factors', conn, if_exists='replace', index=False)
    conn.close()
    
    # 3. ç®€å•å›æµ‹åˆ†æ
    logger.info("Step 3: æ€§èƒ½åˆ†æ")
    performance = backtest.get_performance_summary()
    
    logger.info("=== ç³»ç»Ÿæ€§èƒ½æ‘˜è¦ ===")
    for key, value in performance.items():
        logger.info(f"{key}: {value:.4f}")
    
    # 4. ç”Ÿæˆä¿¡å·æŠ¥å‘Š
    logger.info("Step 4: ç”Ÿæˆäº¤æ˜“ä¿¡å·")
    alpha_df = pd.DataFrame(alpha_results)
    
    # æ ‡å‡†åŒ–ä¿¡å·
    alpha_df['signal'] = np.where(alpha_df['combined_alpha'] > 0.1, 'BUY',
                        np.where(alpha_df['combined_alpha'] < -0.1, 'SELL', 'HOLD'))
    
    logger.info("=== äº¤æ˜“ä¿¡å· ===")
    for _, row in alpha_df.iterrows():
        logger.info(f"{row['ticker']}: {row['signal']} (Alpha: {row['combined_alpha']:.4f})")
    
    logger.info("=== Demoå®Œæˆ ===")
    logger.info(f"æ•°æ®åº“æ–‡ä»¶: {db.db_path}")
    logger.info("å¯ä»¥é€šè¿‡Streamlitç•Œé¢æŸ¥çœ‹è¯¦ç»†ç»“æœ")
    
    return alpha_df, performance

if __name__ == "__main__":
    # è¿è¡ŒDemo
    results, perf = main_demo()
    print("\nDemoè¿è¡Œå®Œæˆï¼")
    print(f"ç”Ÿæˆäº† {len(results)} ä¸ªäº¤æ˜“ä¿¡å·")
    print(f"å¹³å‡IC: {perf['ic_mean']:.4f}")
